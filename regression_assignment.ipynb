{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Simple Linear Regression?\n",
        "\n",
        "Ans.**Simple Linear Regression (SLR)** is a statistical and machine learning technique used to model the relationship between one independent variable (x) and one dependent variable (y) by fitting a straight line.\n",
        "- The regression line:\n",
        "  - y = wx + b\n",
        "  - where x is independent variable(input features)\n",
        "  - y is predicted value or dependent variable\n",
        "  - when w is +ve then there is direct relationship between x and y\n",
        "  - while w is -ve then indirect relationship between x and y\n",
        "  - when w = 0 then no relationship between y and x\n",
        "\n"
      ],
      "metadata": {
        "id": "2UOv9UL2jYbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Ans. Assumptions of simple linear regression are as follows:\n",
        "- **Linearity**: There is linear relationship between X(independent variable) and Y (dependent value).\n",
        "  \n",
        "  --> There is direct or indirect relationship between X and Y.\n",
        "- **Independence**: The observations or rows are indepedent of each others. Since the observations are independent , the error are independent of each other.\n",
        "- **Homoscedasticity (Constant Variance)**: The variance of residuals(errors) is constant across all values of X. It means there is no pattern detected.\n",
        "- **Normality of Errors**: Error should be normally distributed. It is important for hypothesis testing and confidence intervals.\n",
        "- **No Outliers or Influential Points**: Extreme values can distort the regression line. Outliers have a strong impact on slope and intercept.\n",
        "- **No Multicollinearity**: Since there is only one predictor, multicollinearity does not exist."
      ],
      "metadata": {
        "id": "dnyTBiNYjbsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "Ans. The coefficient m represents the slope of the regression line, indicating the change in Y for a one-unit change in X.\n",
        "- When m > 0 then there is direct relationship between X and Y\n",
        "- When m < 0 then there is indirect relationship between X and Y.\n",
        "- When m = 0 then there is no relationship between X and Y."
      ],
      "metadata": {
        "id": "oyc7xXBLjbpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        "Ans. The intercept c value represent the baseline value of dependent variable (when X = 0 then Y=c).\n",
        "- c is the point where the regression line cuts the Y-axis."
      ],
      "metadata": {
        "id": "4hqm_n-pjbmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "Ans. In Simple Linear Regression, the slope m is calculated using the Least Squares Method, which minimizes the sum of squared errors between actual and predicted values.\n",
        "- Formula to calculate slope m:$$m = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}$$\n"
      ],
      "metadata": {
        "id": "d3NrVjfljbjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6.  What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "Ans. The primary purpose of the Least Squares Method is to provide an objective, mathematical way to determine the \"Line of Best Fit.\".\n",
        "- 1. Minimizing Prediction Error: The method ensures the model is as accurate as possible on the training data.\n",
        "It tries to make the difference between the actual values ($y$) and the predicted values ($\\hat{y}$) as small as possible.\n",
        "  error = (Y(predicted) - Y(actual))^2\n",
        "- 2. Penalizing Large Mistakes: If we just added the errors, positive errors would cancel out negative errors, making a terrible line look perfect (sum of 0). Squaring makes everything positive. If there is outliers present in the data then the error become large and affect the regression line that fit int the data.\n",
        "- 3. Mathematical Efficiency:\n",
        "  - Differentiability: Because the \"squared error\" function creates a smooth curve (a parabola) rather than a sharp V-shape (absolute value), we can use calculus to easily find the minimum point.\n"
      ],
      "metadata": {
        "id": "xLmr9vnEjbgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression?\n",
        "\n",
        "Ans.\n",
        "- The Coefficient of Determination, denoted as $R^2$, is the primary metric used to evaluate how well your Linear Regression model fits the data.\n",
        "- $R^2$ represents the proportion of the variance in the dependent variable ($y$) that is predictable from the independent variable ($x$).\n",
        "- It usually ranges from 0 to 1\n",
        "- It compares your model's errors against a \"naive\" model that just predicts the average for everyone\n",
        "- $$R^2 = 1 - \\frac{\\text{Unexplained Variation (Model Error)}}{\\text{Total Variation (Data Spread)}}$$\n",
        "- $R^2 = 1$ (100%): A perfect fit. Every single data point falls exactly on the regression line. This rarely happens with real-world data.\n",
        "- $R^2 = 0$ (0%): The model explains none of the variability. Your model is no better than just guessing the average ($\\bar{y}$) for every prediction."
      ],
      "metadata": {
        "id": "bPe6p4AhjbdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What is Multiple Linear Regression?\n",
        "\n",
        "Ans. Multiple Linear Regression is a supervised learning and statistical technique used to model the relationship between one dependent variable (Y) and two or more independent variables (X‚ÇÅ, X‚ÇÇ, ‚Ä¶, X‚Çô) using a linear equation.\n",
        "- $$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\epsilon$$\n",
        "- Where:\n",
        "  - $y$: The target variable (e.g., House Price).\n",
        "  - $x_1, x_2, \\dots$: The different features (e.g., Size, Age, Location Score).\n",
        "  - $\\beta_0$: The intercept (value of $y$ when all $x$'s are 0).\n",
        "  - $\\beta_1, \\beta_2, \\dots$: The slopes (coefficients) for each specific variable.\n",
        "- In Simple Regression, 10$\\beta$ is the change in 11$y$ for a unit change in 12$x$.13\n",
        "In Multiple Regression, 14$\\beta_1$ is the change in 15$y$ for a unit change in 16$x_1$, holding all other variables (17$x_2, x_3...$) constant.\n",
        "- There is many-to-one relationship."
      ],
      "metadata": {
        "id": "r_be18OUjbZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "Ans.The main difference lies in the number of independent (input) variables used to predict the output.\n",
        "- **Simple Linear Regression**:\n",
        "  - It uses one independent variable X\n",
        "  - Models the relationship between one input (X) and one output (Y).\n",
        "  - Equation: Y = mX + c\n",
        "  - Example: Predicting salary based on years of experience.\n",
        "\n",
        "- **Multiple Linear Regression**:\n",
        "  - It uses two or more independent variables(X1, X2, ...)\n",
        "  - Models the relationship between multiple inputs (X‚ÇÅ, X‚ÇÇ, ‚Ä¶, X‚Çô) and one output (Y).\n",
        "  - Equation: Y = b‚ÇÄ + b‚ÇÅX‚ÇÅ + b‚ÇÇX‚ÇÇ + ‚Ä¶ + b‚ÇôX‚Çô.\n",
        "  - Example: Predicting house price based on size, location, and number of rooms."
      ],
      "metadata": {
        "id": "0gx60MYsjbW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "Ans. The key assumptions of multiple linear regression are as follows:\n",
        "- 1. **Linearity**: The relationship between the independent variables ($X_1, X_2, ... X_n$) and the dependent variable ($Y$) must be linear. This means that a change in $Y$ is proportional to a change in $X$.\n",
        "- 2. **Independence**: The error(the differences between observed and predicted values) must be independent of each other. This is especially important in time-series data, where an observation at one time point might be correlated with a previous one.\n",
        "- 3. **Normality of error**: Errors are normally distributed (important for hypothesis testing). Not strictly required for prediction accuracy, but needed for valid confidence intervals and p-values.\n",
        "- 4. **Homoscedasticity (Constant Variance)**: The variance of the residuals should be constant across all levels of the independent variables.Errors should not fan out or funnel in.\n",
        "- 5. **No Multicollinearity**: Independent variables should not be too highly correlated with each other. If two predictors (like \"Height in inches\" and \"Height in centimeters\") are nearly identical, the model won't be able to tell which one is actually influencing the outcome."
      ],
      "metadata": {
        "id": "6oZ-pogmjbTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Ans. **Heteroscedasticity**: Heteroscedasticity occurs in a Multiple Linear Regression (MLR) model when the variance of the error terms (residuals) is not constant across all levels of the independent variables.\n",
        "- Heteroscedasticity doesn't necessarily make your model's predictions \"wrong\" or biased, but it makes your statistical conclusions unreliable.\n",
        "- ***Inefficient Estimates***: While the coefficients ($\\beta$ values) remain unbiased, they are no longer the \"Best\" (lowest variance) estimates. There is a more precise way to calculate them that the model is missing\n",
        "- ***Invalid p-values and Hypothesis Tests***: This is the biggest danger. Heteroscedasticity biases the calculation of Standard Errors. Since $t$-tests and $F$-tests rely on these standard errors, your p-values will be incorrect.\n",
        "- ***Unreliable Confidence Intervals***: Because the standard errors are off, the \"margin of error\" around your predictions will be too narrow or too wide, leading to a false sense of precision."
      ],
      "metadata": {
        "id": "kHkFTQftjbQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "Ans. High multicollinearity means the independent variables in a Multiple Linear Regression (MLR) model are highly correlated with each other. This makes coefficients unstable and hard to interpret.\n",
        "\n",
        "Ways to Handle High Multicollinearity:-\n",
        "- Remove One of the Correlated Variables: If two variables carry similar information, keep only one. Choose based on domain knowledge or higher correlation with Y.\n",
        "- Combine Correlated Variables: Create a new feature by combining them.\n",
        "Use Averages, sum, ratio, index.\n",
        "- Use Principal Component Analysis (PCA): Convert correlated variables into uncorrelated components.\n",
        "Improves stability but loses interpretability.\n",
        "- Apply Regularization Techniques: Regularization penalizes large coefficients.\n",
        "  - Ridge Regression (L2):Shrinks coefficients but keeps all variables. Very effective for multicollinearity.\n",
        "  - Lasso Regression (L1):Lasso Regression (L1). Performs feature selection.\n",
        "- Increase Sample Size: More data reduces variance of coefficient estimates. Helps when multicollinearity is not perfect.\n",
        "- Center or Standardize Variables: Subtract mean or scale features. Helps with numerical stability, especially with interaction terms.\n",
        "- Check and Remove Redundant Feature: Use VIF (Variance Inflation Factor):\n",
        "  - VIF > 10 ‚Üí serious issue\n",
        "  - VIF 5‚Äì10 ‚Üí moderate issue\n",
        "\n",
        "    Remove features with very high VIF."
      ],
      "metadata": {
        "id": "BfEDd7p5jbNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "Ans. In regression models, categorical variables must be converted into numerical representations. The most commonly used techniques include:\n",
        "- Nominal/One-Hot-Encoding:\n",
        "  - Each category is converted into a binary feature (0/1).\n",
        "  - One category is dropped to act as a reference. If a feature has k categories, OHE creates k‚àí1 binary columns.\n",
        "  - This is for Nominal data where no order exists.\n",
        "  - It prevents the model from assuming a mathematical order between categories where none exists (e.g., preventing a model from thinking \"Green\" is greater than \"Red\")\n",
        "- Label and Ordinal Encoding:\n",
        "  - While OHE is great for linear models, it is often avoided for Tree-based models (like Random Forest, XGBoost, or LightGBM) if the number of categories is high.\n",
        "  - Categories are mapped to integers that preserve order.\n",
        "  - ree-based models can handle these integer values efficiently without the \"dimensionality explosion\" that OHE causes. For ordinal data (e.g., \"Low\", \"Medium\", \"High\"), it preserves the natural rank.\n",
        "  - It is highly memory-efficient because it only adds one numerical feature instead of many.\n",
        "- Target-Guided Ordinal Encoding:\n",
        "  - Each category is replaced with a statistic derived from the target variable (mean, log-odds, etc.).\n",
        "  - Handles high-cardinality categorical variables and very strong predictive power.\n",
        "  - It is highly prone to data leakage and overfitting, so it requires advanced techniques like \"smoothing\" or \"k-fold target encoding\" to be safe for production."
      ],
      "metadata": {
        "id": "AZX5OLovjbKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Ans.\n",
        "- In Multiple Linear Regression, interaction terms are used to model situations where the effect of one independent variable on the outcome depends on the value of another independent variable.\n",
        "- While a standard additive model assumes that each predictor has a constant effect on the target regardless of other factors, interaction terms allow the model to capture more complex, real-world \"joint effects\".\n",
        "- Why Interaction Terms Are Important:\n",
        "  - Capture Conditional Effects: Interaction terms allow the effect of one variable to change depending on another variable.\n",
        "  - Improve Model Accuracy: Many real-world relationships are not purely additive. Interaction terms help:\n",
        "    - Reduce bias\n",
        "    - Capture complex pattern\n",
        "    - Improve explanatory and predictive power\n",
        "  - Enable More Realistic Interpretations:\n",
        "    - Without interaction:Each coefficient is interpreted independently\n",
        "    - With interaction:The effect of ùëã1 depends on ùëã2\n",
        "\t    \n",
        "      **‚àÇùë¶ / ‚àÇùëã1 = ùõΩ1 + ùõΩ3ùëã2**\n",
        "- How It Changes the Regression Equation:\n",
        "  - A standard additive model is written as: $$\\hat{y} = \\beta_0 + \\beta_1X_1 + \\beta_2X_2$$\n",
        "  - A model with a two-way interaction is written as: $$\\hat{y} = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3(X_1 \\cdot X_2)$$\n",
        "  Here, $\\beta_3$ is the coefficient for the interaction term, which is the product of the two original features.\n",
        "\n"
      ],
      "metadata": {
        "id": "zsm9lqCSjbHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15.  How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "Ans. The intercept represents the expected value of the response variable when all predictors are zero, but its practical interpretation differs significantly between Simple Linear Regression (SLR) and Multiple Linear Regression (MLR).\n",
        "- Intercept in Simple Linear Regression (SLR) Model:\n",
        "  - ${y} = \\beta_0 + \\beta_1x$\n",
        "  - Where $\\beta_0$ is the expected value of y when x = 0\n",
        "  - Represents a clear baseline or starting point.\n",
        "- Intercept in Multiple Linear Regression (MLR) Model:\n",
        "  - ${y} = \\beta_0+\\beta_1x_1+\\beta_2x_2 + ....+ \\beta_nx_n$\n",
        "  - $\\beta_0$ is the expected value of y when all the predictors $x_1, x_2,....,x_n$ are zero.\n",
        "- Why Interpretation Is More Difficult\n",
        "  - The combination ‚Äúall predictors = 0‚Äù may be unrealistic or impossible.\n",
        "  - Some predictors may be categorical (encoded as 0/1).\n",
        "  - Zero may lie outside the observed data range.\n",
        "- Why Interpretation Is More Difficult:\n",
        "  - When categorical variables are present:The intercept represents the expected response for the reference group when all numeric predictors are zero.\n",
        "- Centering and Standardization:\n",
        "  - Centering predictors (subtracting the mean) changes the interpretation of the intercept."
      ],
      "metadata": {
        "id": "nqZdP7kpjbEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16.  What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "Ans. Significance of the slope in regression & its effect on predictions\n",
        "- The slope represents the change in the dependent variable for a one-unit increase in the independent variable.\n",
        "- It determines the direction (positive/negative) and strength of the relationship.\n",
        "- Predictions change proportionally based on the slope value."
      ],
      "metadata": {
        "id": "9bxKg5-RjbBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "Ans. Role of the intercept in a regression model:-\n",
        "\n",
        "- The intercept is the expected value of the dependent variable when all predictors are zero.\n",
        "- It provides a baseline or reference point for the relationship.\n",
        "- Helps anchor the regression line in the data space."
      ],
      "metadata": {
        "id": "mVvlhHh6ja-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18.  What are the limitations of using R¬≤ as a sole measure of model performance?\n",
        "\n",
        "Ans. Limitations of using R¬≤ alone for model performance\n",
        "Does not indicate causation.\n",
        "\n",
        "- Always increases when more variables are added, even if irrelevant.\n",
        "- Does not measure prediction accuracy or model bias.\n",
        "- Cannot detect overfitting."
      ],
      "metadata": {
        "id": "-LHOl03Nja7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19.  How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "Ans.\n",
        "Interpretation of a large standard error for a coefficient\n",
        "\n",
        "- Indicates high uncertainty in the coefficient estimate.\n",
        "- Suggests the predictor may be weak, unstable, or affected by multicollinearity.\n",
        "- Reduces confidence in the variable‚Äôs statistical significance."
      ],
      "metadata": {
        "id": "_U8Ejqp6ja4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20.  How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "Ans. Identifying heteroscedasticity in residual plots & why it matters\n",
        "- Identified when residuals show non-constant spread (e.g., funnel shape).\n",
        "- Important because it:\n",
        "    - Violates regression assumptions\n",
        "    - Leads to biased standard errors\n",
        "- Affects hypothesis testing and confidence intervals"
      ],
      "metadata": {
        "id": "-p3CHfacja1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21.  What does it mean if a Multiple Linear Regression model has a high R¬≤ but low adjusted R¬≤?\n",
        "\n",
        "Ans. High R¬≤ but low Adjusted R¬≤ in Multiple Linear Regression\n",
        "- Indicates that added variables do not meaningfully improve the model.\n",
        "- Suggests overfitting or inclusion of irrelevant predictors.\n",
        "- Adjusted R¬≤ penalizes unnecessary complexity.\n"
      ],
      "metadata": {
        "id": "QoY9WWL1jayf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "Ans. Importance of scaling variables in Multiple Linear Regression\n",
        "- Ensures predictors are on comparable scales.\n",
        "\n",
        "- Improves numerical stability and convergence.\n",
        "\n",
        "- Prevents variables with large units from dominating the model.\n",
        "\n",
        "- Essential for regularization methods."
      ],
      "metadata": {
        "id": "EffnsmnujavZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23.  What is polynomial regression?\n",
        "\n",
        "Ans.\n",
        "- Polynomial regression is a form of linear regression where the relationship between the independent variable 13$x$ and the dependent variable 14$y$ is modeled as an 15$n^{th}$ degree polynomial.\n",
        "- Linear Equation: $y = \\beta_0 + \\beta_1x + \\epsilon$\n",
        "- Polynomial Equation: $y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\dots + \\beta_nx^n + \\epsilon$\n",
        "- While linear regression fits a straight line, polynomial regression fits a curve.18 Interestingly, it is still considered \"linear\" regression because it is linear in terms of the coefficients ($\\beta$), even though the features are squared or cubed.\n"
      ],
      "metadata": {
        "id": "GywGg8whjasM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24. How does polynomial regression differ from linear regression?\n",
        "\n",
        "Ans.\n",
        "Difference between polynomial and linear regression\n",
        "- Linear regression fits a straight line.\n",
        "- Polynomial regression fits a curved relationship by adding powers\n",
        "- Polynomial regression is still linear in coefficients, not parameters.\n",
        "- Linear Equation: $y = \\beta_0 + \\beta_1x + \\epsilon$\n",
        "- Polynomial Equation: $y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\dots + \\beta_nx^n + \\epsilon$"
      ],
      "metadata": {
        "id": "doRnxivWjapW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25. When is polynomial regression used?\n",
        "\n",
        "Ans. When to use and LimitationsUse Cases:\n",
        "- When the relationship is curvilinear (e.g., the growth rate of a biological population or the trajectory of a projectile).\n",
        "- Multiple Variables: Yes, it can be applied to multiple variables through \"Interaction Terms\" (e.g., 20$x_1 \\cdot x_2$).\n",
        "- Limitations: It is highly sensitive to outliers and can lead to extreme overfitting at higher degrees, where the curve wiggles to hit every data point but fails to generalize to new data\n",
        "- When data shows a non-linear trend.\n",
        "- When linear regression underfits the data.\n",
        "- Useful for capturing curvature in relationships."
      ],
      "metadata": {
        "id": "cqbN8qsyjamR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q26. What is the general equation for polynomial regression?\n",
        "\n",
        "Ans. The General equation for polynomial regression is\n",
        "Polynomial Equation:\n",
        "- $y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\dots + \\beta_nx^n + \\epsilon$"
      ],
      "metadata": {
        "id": "o8miNZlVjajc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "Ans. Yes, polynomial regression can absolutely be applied to multiple variables. When we move from one independent variable to two or more, it is often referred to as Multivariate Polynomial Regression.\n",
        "- When you have multiple variables (e.g., $x_1$ and $x_2$), the model does not just square each variable individually. It also accounts for the interaction between them.\n",
        "- For a second-degree polynomial with two variables, the equation looks like this:$$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1^2 + \\beta_4x_2^2 + \\beta_5(x_1x_2) + \\epsilon$$\n",
        "- In a multivariate polynomial model, the features are broken down into three types:\n",
        "  - Linear terms: The original features ($x_1, x_2$).\n",
        "  - Power terms: The features raised to a power ($x_1^2, x_2^2$).\n",
        "  - These capture the \"curvature\" of each variable's individual relationship with the target.Interaction terms: The product of features (3$x_1 \\cdot x_2$).these capture how the effect of one variable changes depending on the value of another."
      ],
      "metadata": {
        "id": "si8_SV1ejagd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q28. What are the limitations of polynomial regression?\n",
        "\n",
        "Ans. Limitations of polynomial regression:\n",
        "- High risk of overfitting.\n",
        "\n",
        "- Poor extrapolation beyond data range.\n",
        "\n",
        "- Sensitive to outliers.\n",
        "\n",
        "- Can become computationally complex with high degrees."
      ],
      "metadata": {
        "id": "0uhXezxrjadX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "Ans. Methods to evaluate model fit when choosing polynomial degree\n",
        "\n",
        "- Cross-validation\n",
        "- Adjusted R¬≤\n",
        "- AIC / BIC\n",
        "- Mean Squared Error (MSE)\n",
        "- Residual analysis\n",
        "- Selecting the Degree and ScalingEvaluating\n",
        "- Fit: Use Cross-Validation to see how the model performs on unseen data. You can also monitor the Adjusted $R^2$ or use information criteria like AIC/BIC.\n",
        "- Scaling: In Multiple Linear Regression (and especially Polynomial Regression), scaling (Standardization/Normalization) is crucial. Because $x^3$ grows much faster than $x$, the model might become numerically unstable or give undue weight to higher-degree features."
      ],
      "metadata": {
        "id": "kcWyeCXjjaaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q30. Why is visualization important in polynomial regression?\n",
        "\n",
        "Ans. Visualization: It is the only way to truly \"see\" if the curve makes sense. A model might have a great $R^2$ but produce a curve that is physically impossible for the phenomenon you are studying."
      ],
      "metadata": {
        "id": "WDLgzJGSjaXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q31. How is polynomial regression implemented in Python?\n",
        "\n",
        "Ans. Using scikit-learn, you typically use PolynomialFeatures to transform your data before fitting a standard LinearRegression model:\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "//1. Generate polynomial features (e.g., degree 2)\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "//2. Fit the model\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "model.fit(X_poly, y)\n"
      ],
      "metadata": {
        "id": "OPlGbFkljaUf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-SNer1CPwelc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}