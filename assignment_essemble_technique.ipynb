{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "                                      Theory Questions"
      ],
      "metadata": {
        "id": "Ncg5qy_r8g8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Can we use Bagging for regression problems?\n",
        "\n",
        "Ans.\n",
        "- Yes, Bagging (Bootstrap Aggregating) can absolutely be used for regression problems. In fact, it is a highly effective ensemble technique for reducing variance in predictive models.\n",
        "- When used for regression, the core logic remains the same as in classification, but the way results are aggregated changes."
      ],
      "metadata": {
        "id": "-qMisXwwQYw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the difference between multiple model training and single model training?\n",
        "\n",
        "Ans.\n",
        "- Single model training:\n",
        "  - Single model training involves selecting one specific algorithm (like a Decision Tree, Logistic Regression, or a Neural Network) and training it on the entire training dataset to find the best parameters.\n",
        "  - Advantages:\n",
        "    - Interpretability: It is much easier to explain how a single Linear Regression or Decision Tree reached a conclusion.\n",
        "    - Computationally Cheap: Requires less memory and processing power since only one training process occurs.\n",
        "    - Lower Complexity: Easier to deploy and maintain in a production environment.\n",
        "  - Disadvantages:\n",
        "    - High Risk of Bias or Variance: A single model might be too simple to capture the data (Underfitting) or too complex, capturing noise (Overfitting).\n",
        "    - Sensitivity: If the training data changes slightly, the single model's performance can fluctuate significantly.\n",
        "- Multiple model training (Or Essemble Learning):\n",
        "  - Multiple model training involves training several models and combining their outputs to produce a final prediction.\n",
        "  - This includes techniques like Bagging, Boosting, and Stacking.\n",
        "  - Advantages:\n",
        "    - Higher Accuracy: Often outperforms single models by reducing the overall error.\n",
        "    - Robustness: Because it averages or votes across multiple models, it is less sensitive to outliers or noise in the data.\n",
        "    - Variance/Bias Reduction: Techniques like Bagging reduce variance (overfitting), while Boosting reduces bias (underfitting).\n",
        "  - Disadvantages:\n",
        "    - Black Box Nature: It becomes very difficult to explain why the ensemble made a certain prediction (low interpretability).\n",
        "    - Expensive: Training 100 trees (like in a Random Forest) takes significantly more time and hardware resources than training one."
      ],
      "metadata": {
        "id": "tmjk_CiGRQuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Explain the concept of feature randomness in Random Forest?\n",
        "\n",
        "Ans. In a Random Forest, feature randomness(also known as the \"random subspace method\") is the secret sauce that prevents the model from being just a collection of identical trees. While standard Decision Trees look at every available feature to find the best split, Random Forest restricts each split to a random subset of features.\n",
        "- When building an individual tree within the forest, the algorithm doesn't use all features at once. Instead:\n",
        "  - At each node of the tree, a random sample of $m$ features is chosen from the total $M$ features available.\n",
        "  - The algorithm then searches only within that subset to find the best feature and threshold for the split.\n",
        "  - This process repeats for every node in every tree.\n",
        "  - Typically, for classification tasks, the number of features selected at each split is $m = \\sqrt{M}$. For regression, it is often $m = M/3$.\n",
        "- Feature randomness is a key idea behind Random Forest that helps reduce overfitting and improve generalization.\n",
        "- Needs for the features randomness:\n",
        "  - Reduces correlation between trees:\n",
        "    - Strong features dominate normal trees\n",
        "    - Random feature selection forces trees to be different\n",
        "  - Reduces overfitting:\n",
        "    - Less chance of memorizing noise\n",
        "    - Improves generalization\n",
        "  - Improves ensemble performance:\n",
        "  - Diverse trees more the better averaged prediction."
      ],
      "metadata": {
        "id": "xjuWsc1wRQq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is OOB (Out-of-Bag) Score?\n",
        "\n",
        "Ans.\n",
        "- In a Random Forest, the Out-of-Bag (OOB) Score is a built-in validation method. It allows you to evaluate the model's performance during training without needing a separate validation set or performing cross-validation.\n",
        "- In simple we say that OOB score is a built-in way to estimate the accuracy of a Random Forest without using a separate validation set.\n",
        "- Random Forest uses bootstrap sampling:\n",
        "  - Each tree is trained on a random sample drawn with replacement\n",
        "  - About 63% of data points are used to train a tree\n",
        "  - The remaining ~37% are not used → these are out-of-bag samples"
      ],
      "metadata": {
        "id": "BPhEjhfZRQoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. How can you measure the importance of features in a Random Forest model?\n",
        "\n",
        "Ans.\n",
        "- In a Random Forest, measuring feature importance helps you understand which variables are doing the \"heavy lifting\" to make accurate predictions.\n",
        "- There are two primary ways the algorithm calculates this internally.\n",
        "- Mean Decrease in Impurity (Gini Importance): This is the default method used by most libraries (like scikit-learn). It measures how much each feature reduces the \"impurity\" of the nodes it splits.\n",
        "  - How it works: Every time a node is split on a specific feature, the Gini impurity (for classification) or Mean Squared Error (for regression) decreases.\n",
        "  - Calculation: The algorithm sums up all these decreases for a feature across all trees in the forest and averages them.\n",
        "  - Average and normalize the result: Importance(f) = ∑ ΔImpurity\n",
        "  - Pros: It is very fast to calculate.\n",
        "  - Cons: It can be biased toward high-cardinality features (features with many unique values, like ZIP codes or IDs), making them seem more important than they actually are.\n",
        "  - Interpretation:\n",
        "    - Larger decrease --> more important feature\n",
        "    - Values are normalized to sum to 1\n",
        "- Mean Decrease in Accuracy (Permutation Importance): This method is often considered more reliable because it directly measures how much the model depends on a feature for its final performance.\n",
        "  - How it works:\n",
        "    - 1. The model is trained normally.\n",
        "    - 2. For a specific feature, the values in that column are shuffled (permuted) randomly, breaking any relationship with the target.\n",
        "    - 3. The model makes predictions on this \"corrupted\" data.\n",
        "  - Calculation: The drop in the model's performance (e.g., accuracy or $R^2$) is recorded. If the accuracy drops significantly, the feature is highly important.\n",
        "  - Pros: It is robust and does not favor high-cardinality features.\n",
        "  - Cons: It is more computationally expensive because it requires re-evaluating the model for every feature.\n",
        "  - Computation:\n",
        "    - Measure baseline accuracy (often using OOB score).\n",
        "    - Randomly permute one feature across samples.\n",
        "    - Recompute accuracy.\n",
        "    - Drop in accuracy = feature importance :  Importance(f) = Accuracy(baseline) ​− Accuracy(permuted f)\n",
        "  - Interpretation: Bigger accuracy drop --> more important feature"
      ],
      "metadata": {
        "id": "we7HXJMERQik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Explain the working principle of a Bagging Classifier?\n",
        "\n",
        "Ans.\n",
        "- A Bagging classifier is an ensemble learning method that improves prediction accuracy by training multiple models on different samples of the data and combining their outputs.\n",
        "- Its main goal is to reduce variance and prevent overfitting, especially for unstable models like decision trees.\n",
        "- Core idea: Instead of training one model on the full dataset, bagging:\n",
        "  - Trains many models\n",
        "  - Each on a different random subset of the data\n",
        "  - Aggregates their predictions\n",
        "- Working principle:\n",
        "  - Bootstrap sampling:\n",
        "    - From the original training dataset of size N.\n",
        "    - Create multiple new datasets by sampling with replacement\n",
        "    - Each bootstrap dataset is also of size N.\n",
        "  - Train base learners:\n",
        "    - Train one base classifier on each bootstrap sample\n",
        "    - Base learners are usually decision trees\n",
        "    - Each model learns slightly different patterns.\n",
        "  - Make predictions:\n",
        "    - For a new data point: Each trained model gives a prediction\n",
        "  - Aggregate predictions:\n",
        "    - Classification: Majority voting\n",
        "    - Regression: Average of predictions\n",
        "\n",
        "    This aggregation reduces errors caused by individual models."
      ],
      "metadata": {
        "id": "6Rrv6vNHRQWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. How do you evaluate a Bagging Classifier’s performance?\n",
        "\n",
        "Ans. To evaluate a Bagging Classifier’s performance, we use standard classification metrics, plus a few methods that are especially useful for bagging.\n",
        "- Test Set / Validation Set Evaluation:\n",
        "  - How it works\n",
        "    - Split data into training and test sets\n",
        "    - Train the bagging classifier on training data\n",
        "    - Evaluate predictions on test data\n",
        "  - Common metrics:\n",
        "    - Accuracy: The percentage of total guesses that were correct.\n",
        "    - Precision: Out of all predicted positives, how many were actually positive?\n",
        "    - Recall: Out of all actual positives, how many did the model find?\n",
        "    - F1-score: The harmonic mean of Precision and Recall; great for imbalanced classes.\n",
        "    - Confusion matrix: It maps the Actual values against the Predicted values, making it easy to see exactly where the model is getting \"confused.\"\n",
        "- Cross-Validation:\n",
        "  - How it works:\n",
        "    - Perform k-fold cross-validation\n",
        "    - Train bagging classifier on K-1 folds.\n",
        "    - Test on remaining fold\n",
        "    - Average performance across folds\n",
        "  - Why useful:\n",
        "    - Reliable performance estimate\n",
        "    - Reduces dependence on one split\n",
        "- Out-of-Bag (OOB) Evaluation (Most Important for Bagging):\n",
        "  - How it works\n",
        "    - Each model is trained on a bootstrap sample\n",
        "    - About 37% of data is left out (OOB samples)\n",
        "    - For each data point:\n",
        "      - Use only models where that point was OOB\n",
        "      - Aggregate predictions and compute accuracy\n",
        "  - Why OOB is powerful:\n",
        "    - No separate validation set needed\n",
        "    - Efficient use of data\n",
        "    - Unbiased performance estimate\n",
        "- ROC-AUC Curves:\n",
        "  - For binary classification, the Receiver Operating Characteristic (ROC) curve helps you understand the trade-off between the True Positive Rate and False Positive Rate across different thresholds.\n",
        "  - AUC (Area Under the Curve): A higher AUC (closer to 1.0) indicates the model is excellent at distinguishing between the two classes.\n",
        "  - If your Bagging Classifier is performing poorly on the training set, it likely has a bias problem. Since Bagging only fixes variance, you might need to switch to a more complex base learner or try a Boosting algorithm instead."
      ],
      "metadata": {
        "id": "9XpwD4iaRQTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. How does a Bagging Regressor work?\n",
        "\n",
        "Ans. A Bagging Regressor (Bootstrap Aggregating for regression) is an ensemble method that improves prediction accuracy by reducing variance through averaging multiple regression models.\n",
        "- It is especially effective for high-variance regressors like decision trees.\n",
        "- Instead of training one regression model, bagging:\n",
        "  - Trains many regressors\n",
        "  - Each on a different bootstrap sample of the data\n",
        "  - Combines their predictions by averaging\n",
        "- Working principles:\n",
        "  - Bootstrap sampling:\n",
        "    - From the original dataset of size N\n",
        "    - Create multiple datasets by sampling with replacement\n",
        "    - Each bootstrap sample has size N\n",
        "    - Some data points repeat, some are left out (OOB).\n",
        "  - Train base regressors:\n",
        "    - Train one base regressor on each bootstrap sample\n",
        "    - Base models are often: Decision tree regressors\n",
        "    - Each regressor learns slightly different patterns.\n",
        "  - Make predictions:\n",
        "    - For a new input: Each regressor outputs a numerical prediction\n",
        "  - Aggregate predictions:\n",
        "    - Final prediction = average of all regressor outputs\n",
        "    - y ​= 1/M​ ∑ ​y​i​\n",
        "    - where M is number of regresssor model.\n",
        "- Performance evaluation:\n",
        "  - Mean Squared Error(MSE)\n",
        "  - Root Mean Squared Error(RMSE)\n",
        "  - R2 score\n",
        "  - Out-of-Bag(OOB) error\n"
      ],
      "metadata": {
        "id": "7WQH7JHfRQQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. What is the main advantage of ensemble techniques?\n",
        "\n",
        "Ans. Advantage of Ensemble Techniques:\n",
        "- The main advantage of ensemble techniques is that they combine multiple models to produce better predictions than any single model.\n",
        "- Many weak or average models together make a strong, reliable model.\n",
        "- Different models make different errors. When combined, these errors cancel out, leading to better performance.\n",
        "- Improved accuracy:\n",
        "  - Ensembles usually outperform individual models\n",
        "  - More robust predictions\n",
        "- Reduced overfitting:\n",
        "  - Averaging (bagging) reduces variance\n",
        "  - Boosting reduces bias\n",
        "- Better generalization:\n",
        "  - Performs well on unseen data\n",
        "  - Less sensitive to noise\n",
        "- Increased stability: Small data changes don’t affect predictions much.\n",
        "- Bias–Variance perspective:\n",
        "  - Bagging -> reduces variance\n",
        "  - Boosting -> reduces bias\n",
        "  - Ensembles -> better bias–variance balance"
      ],
      "metadata": {
        "id": "mYZFz5XURQMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. What is the main challenge of ensemble methods?\n",
        "\n",
        "Ans. The main challenge of ensemble methods is their high computational cost and complexity.\n",
        "- Ensemble methods:\n",
        "  - Train many models instead of one\n",
        "  - Require more time, memory, and processing power\n",
        "  - Are harder to interpret than single models\n",
        "- Key challenges:\n",
        "  - Increased computational cost\n",
        "    - Multiple models must be trained\n",
        "    - Prediction time is slower\n",
        "    - Not ideal for real-time or low-resource systems\n",
        "  - Reduced interpretability\n",
        "    - Hard to understand decisions\n",
        "    - Difficult to explain to users (black-box behavior)\n",
        "  - Risk of diminishing returns\n",
        "    - Adding more models doesn’t always improve performance\n",
        "    - Poorly chosen models add noise, not value\n",
        "  - Data and tuning complexity\n",
        "    - More hyperparameters to tune\n",
        "    - Requires careful design to avoid redundancy\n"
      ],
      "metadata": {
        "id": "mhwjJXB9RQJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. Explain the key idea behind ensemble techniques.\n",
        "\n",
        "Ans. The key idea behind ensemble techniques is to combine multiple models so that their collective decision is more accurate and reliable than any single model.\n",
        "- Each model learns something slightly different, and when their predictions are combined, individual errors tend to cancel out.\n",
        "- combining models helps:\n",
        "  - Different models make different errors\n",
        "  - Errors are often uncorrelated\n",
        "  - Aggregation(voting or averaging) reduces overall error\n",
        "- predictions are combined:\n",
        "  - Classification: In classification, each model predicts a class label. The final output is the class that gets the most votes(Majority voting)\n",
        "  - Regression: In regression, each model predicts a numerical value. The final output is the average of all predictions(Averaging predictions)\n",
        "- Essemble technique used:\n",
        "  - Bagging\n",
        "    - Parallel models\n",
        "    - Reduces variance\n",
        "  - Boosting\n",
        "    - Sequential models\n",
        "    - Reduces bias\n",
        "  - Stacking\n",
        "    - Meta-model learns how to combine predictions"
      ],
      "metadata": {
        "id": "Xe-Jrc6lRQHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12. What is a Random Forest Classifier?\n",
        "\n",
        "Ans. A Random Forest Classifier is a machine-learning model that combines many decision trees to make more accurate and stable predictions.\n",
        "- Instead of relying on a single tree(which can cause overfiting), it builds a forest of trees and lets them vote on the final class.\n",
        "- How it works:\n",
        "- Bootstrap sampling: Each tree is trained on a random sample of the training data (sampling with replacement).\n",
        "- Random feature selection: At every split, a tree looks at only a random subset of features, not all of them.\n",
        "- Ensemble voting: For classification, each tree predicts a class -> the class with the most votes wins."
      ],
      "metadata": {
        "id": "CfZpS7AWRQDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13. What are the main types of ensemble techniques?\n",
        "\n",
        "Ans. The main types of ensemble techniques are methods that combine multiple models to get better performance than any single model alone. There are four core types you’ll see most often.\n",
        "- Bagging (Bootstrap Aggregating):\n",
        "  - Idea: Train many models independently on different random samples of the data, then average or vote.\n",
        "  - Why it works: it reduces variance and helps prevent overfitting.\n",
        "  - Common examples\n",
        "    - Random Forest (a smarter form of bagging)\n",
        "    - BaggingClassifier / BaggingRegressor\n",
        "  - Best for High-variance models (like decision trees)\n",
        "\n",
        "- Boosting:\n",
        "  - Idea: Train models sequentially, where each new model focuses more on the mistakes made by previous ones.\n",
        "  - Why it works: It reduces bias and builds strong learners from weak ones.\n",
        "  - Common examples\n",
        "    - AdaBoost\n",
        "    - Gradient Boosting\n",
        "    - XGBoost, LightGBM, CatBoost\n",
        "  - Best for Complex patterns, High predictive accuracy\n",
        "- Stacking (Stacked Generalization):\n",
        "  - Idea: Train different types of models and then train a meta-model to combine their predictions.\n",
        "  - Why it works: It learns how to best combine models instead of just averaging them.\n",
        "  - Common examples: Decision Tree + SVM + Logistic Regression, Random Forest + Linear Model\n",
        "  - Best for Combining diverse models, Squeezing out extra performance\n",
        "- Voting / Averaging\n",
        "  - Idea: Combine predictions directly using majority vote (classification) or mean (regression).\n",
        "  - Types:\n",
        "    - Hard voting -> class labels\n",
        "    - Soft voting -> predicted probabilities\n",
        "  - Best for Simple ensembles, Quick performance boost with minimal complexity"
      ],
      "metadata": {
        "id": "33AJ0kcERQAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14. What is ensemble learning in machine learning?\n",
        "\n",
        "Ans.\n",
        "- Ensemble learning is a machine-learning approach where multiple models are combined to solve the same problem and produce better predictions than any single model on its own.\n",
        "- Instead of trusting one model, ensemble learning lets several models work together.\n",
        "- How ensemble learning works:\n",
        "  - Train multiple models (often called base learners)\n",
        "  - Each model makes its own prediction\n",
        "  - Predictions are combined using:\n",
        "  - Voting (classification)\n",
        "  - Averaging (regression)\n",
        "  - Weighted combinations or a meta-model"
      ],
      "metadata": {
        "id": "b0AA_VO5RP9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15. When should we avoid using ensemble methods?\n",
        "\n",
        "Ans. Ensemble methods are powerful, but they’re not always the right choice. You should avoid or be cautious about using them in these situations:\n",
        "- 1. When interpretability is critical:\n",
        "  - Ensembles (like Random Forests or Boosting) act like black boxes.\n",
        "  - Avoid if You must clearly explain why a prediction was made, You’re working in regulated domains (finance, healthcare, law)\n",
        "  - Prefer: linear models or a single decision tree\n",
        "\n",
        "- 2. When the dataset is very small:\n",
        "  - Ensembles need enough data to benefit from diversity.\n",
        "  - Avoid if:\n",
        "    - You have very few samples\n",
        "    - Models start memorizing instead of generalizing\n",
        "  - Prefer: simpler models with regularization\n",
        "\n",
        "- 3. When computational resources are limited:\n",
        "  - Ensembles are heavier in:\n",
        "    - Training time\n",
        "    - Memory usage\n",
        "    - Inference latency\n",
        "  - Avoid if:\n",
        "    - You’re on edge devices.\n",
        "    - You need real-time predictions\n",
        "  - Prefer: lightweight models\n",
        "\n",
        "- 4. When a simple model already performs well\n",
        "  - If a basic model is accurate and stable, ensembles may add unnecessary complexity.\n",
        "  - Avoid if:\n",
        "    - Performance gain is marginal\n",
        "    - Maintenance cost outweighs benefits\n",
        "\n",
        "- 5. When data is highly noisy:\n",
        "  - Some ensemble methods (especially boosting) can over-focus on noise.\n",
        "  - Avoid if:\n",
        "    - Labels are unreliable\n",
        "    - Data has many outliers\n",
        "  - Prefer: robust or regularized models\n",
        "\n",
        "- 6. When fast model updates are required\n",
        "  - Ensembles are slower to retrain and tune.\n",
        "  - Avoid if:\n",
        "    - Data changes frequently\n",
        "    - You need quick retraining cycles"
      ],
      "metadata": {
        "id": "sr4dRVqBRP6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16. How does Bagging help in reducing overfitting?\n",
        "\n",
        "Ans. Bagging reduces overfitting by training multiple models on different bootstrap samples and averaging their predictions, which lowers variance and stabilizes results.\n",
        "- What Bagging does:\n",
        "  - 1.Creates many different training sets: It uses bootstrap sampling (random sampling with replacement) to generate multiple datasets from the original data.\n",
        "  - 2.Trains a model on each dataset: Each model sees a slightly different version of the data, so they make different mistakes.\n",
        "  - 3.Aggregates predictions:\n",
        "    - Classification -> majority vote\n",
        "    - Regression -> average"
      ],
      "metadata": {
        "id": "rxEO-mUlRP3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17. Why is Random Forest better than a single Decision Tree?\n",
        "\n",
        "Ans. Random Forest is better than a single Decision Tree because:\n",
        "- Reduces overfitting by averaging many trees\n",
        "- Lower variance and more stable predictions\n",
        "- Better generalization on unseen data\n",
        "- Handles noise and outliers more robustly\n",
        "- Uses random feature selection, making trees less correlated\n",
        "- Provides higher accuracy in most real-world problems"
      ],
      "metadata": {
        "id": "HWGH1aYlRP06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18. What is the role of bootstrap sampling in Bagging?\n",
        "\n",
        "Ans. Bootstrap sampling is the key mechanism that makes Bagging work.\n",
        "- Role of bootstrap sampling in Bagging:\n",
        "  - Creates multiple different training datasets from the original data\n",
        "  - Sampling is done with replacement, so each dataset is slightly different\n",
        "  - Introduces diversity among the base models\n",
        "  - Ensures models make different errors, not the same ones\n",
        "  - Allows averaging/voting to reduce variance and overfitting\n",
        "- Without bootstrap sampling, all models would see the same data and behave almost identically—Bagging would give no benefit."
      ],
      "metadata": {
        "id": "0PL5nTFtRPx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19. What are some real-world applications of ensemble techniques?\n",
        "\n",
        "Ans. Ensemble techniques are used wherever accuracy, robustness, and reliability really matter. Here are some real-world applications.\n",
        "- Finance:\n",
        "  - Credit scoring – combine models to predict loan default risk\n",
        "  - Fraud detection – ensembles catch subtle and rare fraud patterns\n",
        "  - Stock market prediction – reduce noise in financial data\n",
        "- Healthcare:\n",
        "  - Disease diagnosis – cancer detection, heart disease prediction\n",
        "  - Medical imaging – ensembles improve accuracy in X-ray/MRI analysis\n",
        "  - Patient risk prediction – ICU readmission, survival analysis\n",
        "- E-commerce & Marketing:\n",
        "  - Recommendation systems – product and content recommendations\n",
        "  - Customer churn prediction – identify users likely to leave\n",
        "  - Ad click-through prediction – improve targeting accuracy"
      ],
      "metadata": {
        "id": "Ec5_YF7ORPu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20. What is the difference between Bagging and Boosting?\n",
        "\n",
        "Ans. The difference between Bagging and Boosting are as follow:\n",
        "\n",
        "- Bagging (Bootstrap Aggregating):\n",
        "  - Models are trained independently\n",
        "  - Training happens in parallel\n",
        "  - Uses random bootstrap samples\n",
        "  - Focuses on reducing variance\n",
        "  - Works best with high-variance models (e.g., decision trees)\n",
        "  - Less sensitive to noise\n",
        "  - Example: Random Forest\n",
        "\n",
        "- Boosting:\n",
        "  - Models are trained sequentially\n",
        "  - Each new model focuses more on previous errors\n",
        "  - Uses weighted samples\n",
        "  - Focuses on reducing bias\n",
        "  - Can overfit if data is noisy\n",
        "  - Example: AdaBoost, Gradient Boosting, XGBoost"
      ],
      "metadata": {
        "id": "wwMkLa2gRPsO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "                                 Practical Questions"
      ],
      "metadata": {
        "id": "vL1g3uLvRPFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a sample dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Base estimator\n",
        "base_tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Bagging classifier\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=base_tree,\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqoNqyDgRObR",
        "outputId": "76ba74cd-bd58-4579-ed9e-3fa22f632613"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load a sample regression dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Base regressor\n",
        "base_tree = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Bagging regressor\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=base_tree,\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6aqzXrirDR7",
        "outputId": "fec4a55f-9415-468a-bc14-d81a5ffcc2ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.2559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf_clf.feature_importances_\n",
        "\n",
        "# Display feature importance scores\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": importances\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(feature_importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9tKP_ZFrZK6",
        "outputId": "f9dbf415-915f-417a-f541-6f5fe6f08a38"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    Feature  Importance\n",
            "23               worst area    0.153892\n",
            "27     worst concave points    0.144663\n",
            "7       mean concave points    0.106210\n",
            "20             worst radius    0.077987\n",
            "6            mean concavity    0.068001\n",
            "22          worst perimeter    0.067115\n",
            "2            mean perimeter    0.053270\n",
            "0               mean radius    0.048703\n",
            "3                 mean area    0.047555\n",
            "26          worst concavity    0.031802\n",
            "13               area error    0.022407\n",
            "21            worst texture    0.021749\n",
            "25        worst compactness    0.020266\n",
            "10             radius error    0.020139\n",
            "5          mean compactness    0.013944\n",
            "1              mean texture    0.013591\n",
            "12          perimeter error    0.011303\n",
            "24         worst smoothness    0.010644\n",
            "28           worst symmetry    0.010120\n",
            "16          concavity error    0.009386\n",
            "4           mean smoothness    0.007285\n",
            "19  fractal dimension error    0.005321\n",
            "15        compactness error    0.005253\n",
            "29  worst fractal dimension    0.005210\n",
            "11            texture error    0.004724\n",
            "14         smoothness error    0.004271\n",
            "18           symmetry error    0.004018\n",
            "9    mean fractal dimension    0.003886\n",
            "8             mean symmetry    0.003770\n",
            "17     concave points error    0.003513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q24. Train a Random Forest Regressor and compare its performance with a single Decision Tree.\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load regression dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1\n",
        ")\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "dt_reg = DecisionTreeRegressor(random_state=1)\n",
        "dt_reg.fit(X_train, y_train)\n",
        "dt_pred = dt_reg.predict(X_test)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "\n",
        "# Evaluate models\n",
        "dt_mse = mean_squared_error(y_test, dt_pred)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "print(f\"Decision Tree MSE: {dt_mse:.4f}\")\n",
        "print(f\"Random Forest MSE: {rf_mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_JnNoZuroF5",
        "outputId": "790c43c8-61d5-4364-edc1-902e3071e4e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree MSE: 0.5013\n",
            "Random Forest MSE: 0.2542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train Random Forest with OOB enabled\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    oob_score=True,\n",
        "    bootstrap=True,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_clf.fit(X, y)\n",
        "\n",
        "# Print OOB score\n",
        "print(f\"OOB Score: {rf_clf.oob_score_:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X200LZ5br_ss",
        "outputId": "d43ad2bf-d461-47de-aeca-aacce0ae6cef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB Score: 0.9596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q26. Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Base estimator (SVM)\n",
        "svm = SVC(kernel=\"rbf\", probability=True, random_state=42)\n",
        "\n",
        "# Bagging Classifier with SVM\n",
        "bagging_svm = BaggingClassifier(\n",
        "    estimator=svm,\n",
        "    n_estimators=20,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train model\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu0XLUnFsKGz",
        "outputId": "12b8d21f-2d03-47d1-ff70-2cdf4e6bbdea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q27. Train a Random Forest Classifier with different numbers of trees and compare accuracy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Different numbers of trees\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "\n",
        "for n_estimators in n_estimators_list:\n",
        "    rf_clf = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Trees: {n_estimators:3d} | Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6sS3Mx2sWBo",
        "outputId": "963f1cc8-92e1-4272-d5f7-cbee9bd71cb0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trees:  10 | Accuracy: 0.9561\n",
            "Trees:  50 | Accuracy: 0.9649\n",
            "Trees: 100 | Accuracy: 0.9649\n",
            "Trees: 200 | Accuracy: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Base estimator (Logistic Regression)\n",
        "log_reg = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    solver=\"liblinear\"\n",
        ")\n",
        "\n",
        "# Bagging Classifier\n",
        "bagging_lr = BaggingClassifier(\n",
        "    estimator=log_reg,\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train model\n",
        "bagging_lr.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = bagging_lr.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# AUC score\n",
        "auc = roc_auc_score(y_test, y_prob)\n",
        "print(f\"AUC Score: {auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRZIFxUesmRe",
        "outputId": "b0572923-4b18-4f4d-e7e5-efc63de1a61f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Score: 0.9950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q29. Train a Random Forest Regressor and analyze feature importance scores.\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    \"Feature\": X.columns,\n",
        "    \"Importance\": rf_reg.feature_importances_\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(\"Random Forest Feature Importances:\")\n",
        "print(feature_importance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HEb_D5Qs6gT",
        "outputId": "8f034082-2d0e-4b18-c2a0-7c4ee2acbc8d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Feature Importances:\n",
            "      Feature  Importance\n",
            "0      MedInc    0.524871\n",
            "5    AveOccup    0.138443\n",
            "6    Latitude    0.088936\n",
            "7   Longitude    0.088629\n",
            "1    HouseAge    0.054593\n",
            "2    AveRooms    0.044272\n",
            "4  Population    0.030650\n",
            "3   AveBedrms    0.029606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q30. Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "bagging_acc = accuracy_score(y_test, bagging_clf.predict(X_test))\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_clf.fit(X_train, y_train)\n",
        "rf_acc = accuracy_score(y_test, rf_clf.predict(X_test))\n",
        "\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_acc:.4f}\")\n",
        "print(f\"Random Forest Accuracy:     {rf_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inL0Gb5RtUB0",
        "outputId": "d76356d2-c2c9-47f9-803c-a0913fc5d355"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 0.9561\n",
            "Random Forest Accuracy:     0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q31.  Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 200, 300],\n",
        "    \"max_depth\": [None, 5, 10, 20],\n",
        "    \"min_samples_split\": [2, 5, 7]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXLHockGtd_b",
        "outputId": "ae13c316-0e31-481c-9095-d6b3a5659d68"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Test Accuracy: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q32. Train a Bagging Regressor with different numbers of base estimators and compare performance\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Different numbers of estimators\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "\n",
        "for n_estimators in n_estimators_list:\n",
        "    bagging_reg = BaggingRegressor(\n",
        "        estimator=DecisionTreeRegressor(random_state=42),\n",
        "        n_estimators=n_estimators,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f\"Estimators: {n_estimators:3d} | MSE: {mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15HykGXItrMe",
        "outputId": "c9b86be0-f3e6-4ae1-fa93-0f35b3d95bd5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimators:  10 | MSE: 0.2824\n",
            "Estimators:  50 | MSE: 0.2573\n",
            "Estimators: 100 | MSE: 0.2559\n",
            "Estimators: 200 | MSE: 0.2541\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q33. Train a Random Forest Classifier and analyze misclassified samples.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Find misclassified samples\n",
        "misclassified_idx = y_pred != y_test\n",
        "\n",
        "misclassified_samples = pd.DataFrame(\n",
        "    X_test[misclassified_idx],\n",
        "    columns=data.feature_names\n",
        ")\n",
        "misclassified_samples[\"True Label\"] = y_test[misclassified_idx]\n",
        "misclassified_samples[\"Predicted Label\"] = y_pred[misclassified_idx]\n",
        "\n",
        "print(f\"Number of misclassified samples: {misclassified_samples.shape[0]}\")\n",
        "print(misclassified_samples.head())"
      ],
      "metadata": {
        "id": "9u5IaMk5uPQd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier.\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# Bagging Classifier\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "bag_pred = bagging.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "print(f\"Decision Tree Accuracy: {dt_acc:.4f}\")\n",
        "print(f\"Bagging Accuracy:       {bag_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98-M44_Iux7i",
        "outputId": "2a10a4e6-99e0-4298-b708-d64c8f0810a0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.9474\n",
            "Bagging Accuracy:       0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q35. Train a Random Forest Classifier and visualize the confusion matrix.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "DAEM_Jsbu5Ve",
        "outputId": "fcbe6022-dedc-440e-b3c0-5a2bdaef455e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Base models\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "svm = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm\", SVC(kernel=\"rbf\", probability=True, random_state=42))\n",
        "])\n",
        "\n",
        "lr = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"lr\", LogisticRegression(max_iter=5000))\n",
        "])\n",
        "\n",
        "# Stacking Classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        (\"dt\", dt),\n",
        "        (\"svm\", svm),\n",
        "        (\"lr\", lr)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(max_iter=5000),\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train model\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyRtzJU2vSsX",
        "outputId": "19e59e01-02f7-4739-fb38-4093db5ac4d8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier Accuracy: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q37. Train a Random Forest Classifier and print the top 5 most important features.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "feature_names = data.feature_names\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "feature_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": importances\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_df.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqGM-42VvUmm",
        "outputId": "1f095eb8-6d64-4b3f-957a-e3d6f2145b91"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.153892\n",
            "27  worst concave points    0.144663\n",
            "7    mean concave points    0.106210\n",
            "20          worst radius    0.077987\n",
            "6         mean concavity    0.068001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score.\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Bagging classifier\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "bagging.fit(X_train, y_train)\n",
        "bag_pred = bagging.predict(X_test)\n",
        "\n",
        "# Metrics\n",
        "precision = precision_score(y_test, bag_pred)\n",
        "recall = recall_score(y_test, bag_pred)\n",
        "f1 = f1_score(y_test, bag_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-score:  {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4tzG0PjvW6g",
        "outputId": "e3cf11ad-d319-4c27-8214-34adeed7648d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9583\n",
            "Recall:    0.9718\n",
            "F1-score:  0.9650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "depths = [None, 3, 5, 10, 20]\n",
        "\n",
        "print(\"Effect of max_depth on Accuracy\")\n",
        "for depth in depths:\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=depth,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    rf.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, rf.predict(X_test))\n",
        "    print(f\"max_depth={depth} → Accuracy={acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hz6uS24vxyu",
        "outputId": "6b7fd477-789d-4ced-df5b-06a3f8530d95"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effect of max_depth on Accuracy\n",
            "max_depth=None → Accuracy=0.9649\n",
            "max_depth=3 → Accuracy=0.9649\n",
            "max_depth=5 → Accuracy=0.9649\n",
            "max_depth=10 → Accuracy=0.9649\n",
            "max_depth=20 → Accuracy=0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance.\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "models = {\n",
        "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
        "    \"KNN\": KNeighborsRegressor()\n",
        "}\n",
        "\n",
        "print(\"\\nBagging Regressor Performance\")\n",
        "for name, base_model in models.items():\n",
        "    bag = BaggingRegressor(\n",
        "        estimator=base_model,\n",
        "        n_estimators=50,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    bag.fit(X_train, y_train)\n",
        "    mse = mean_squared_error(y_test, bag.predict(X_test))\n",
        "    print(f\"{name} Base → MSE={mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na6KWZBJw-ne",
        "outputId": "7a7b914d-d9e3-4d17-ee92-460da8dec9aa"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bagging Regressor Performance\n",
            "Decision Tree Base → MSE=0.2573\n",
            "KNN Base → MSE=1.0763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score.\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "rf_auc = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_auc.fit(X_train, y_train)\n",
        "y_prob = rf_auc.predict_proba(X_test)[:, 1]\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(f\"\\nRandom Forest ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuNweL1ExAVv",
        "outputId": "38738b16-aa7b-40e2-f2d2-1dc23583f248"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Random Forest ROC-AUC Score: 0.9953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q42. Train a Bagging Classifier and evaluate its performance using cross-validation.\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "cv_scores = cross_val_score(\n",
        "    bagging_clf, X, y, cv=5, scoring=\"accuracy\"\n",
        ")\n",
        "\n",
        "print(\"\\nBagging Classifier Cross-Validation Accuracy\")\n",
        "print(f\"Mean Accuracy: {cv_scores.mean():.4f}\")\n",
        "print(f\"Std Dev:       {cv_scores.std():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrOI0pi6xCfz",
        "outputId": "d34f331a-02e6-4616-f0d0-043fe90246e1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bagging Classifier Cross-Validation Accuracy\n",
            "Mean Accuracy: 0.9579\n",
            "Std Dev:       0.0382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q43. Train a Random Forest Classifier and plot the Precision-Recall curve.\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rf_pr = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_pr.fit(X_train, y_train)\n",
        "y_scores = rf_pr.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve (Random Forest)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "_EgpB3fqxEE-",
        "outputId": "9b26d3ae-78ef-4e06-d813-de02dd1fb0e4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Logistic Regression (with scaling)\n",
        "lr = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"lr\", LogisticRegression(max_iter=5000))\n",
        "])\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Stacking Classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        (\"rf\", rf),\n",
        "        (\"lr\", lr)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(max_iter=5000),\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train models\n",
        "rf.fit(X_train, y_train)\n",
        "lr.fit(X_train, y_train)\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "rf_acc = accuracy_score(y_test, rf.predict(X_test))\n",
        "lr_acc = accuracy_score(y_test, lr.predict(X_test))\n",
        "stack_acc = accuracy_score(y_test, stacking_clf.predict(X_test))\n",
        "\n",
        "print(\"Accuracy Comparison\")\n",
        "print(f\"Random Forest Accuracy: {rf_acc:.4f}\")\n",
        "print(f\"Logistic Regression Accuracy: {lr_acc:.4f}\")\n",
        "print(f\"Stacking Classifier Accuracy: {stack_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13Y6AX1yxzaR",
        "outputId": "afea013d-5550-412c-897c-97dad2b9e9f6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Comparison\n",
            "Random Forest Accuracy: 0.9561\n",
            "Logistic Regression Accuracy: 0.9825\n",
            "Stacking Classifier Accuracy: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "configs = [\n",
        "    {\"bootstrap\": True, \"max_samples\": 1.0},\n",
        "    {\"bootstrap\": True, \"max_samples\": 0.7},\n",
        "    {\"bootstrap\": False, \"max_samples\": 1.0}\n",
        "]\n",
        "\n",
        "print(\"\\nBagging Regressor Performance (MSE)\")\n",
        "for cfg in configs:\n",
        "    bagging_reg = BaggingRegressor(\n",
        "        estimator=DecisionTreeRegressor(random_state=42),\n",
        "        n_estimators=100,\n",
        "        bootstrap=cfg[\"bootstrap\"],\n",
        "        max_samples=cfg[\"max_samples\"],\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    preds = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, preds)\n",
        "\n",
        "    print(\n",
        "        f\"bootstrap={cfg['bootstrap']}, \"\n",
        "        f\"max_samples={cfg['max_samples']} : MSE={mse:.4f}\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPlAs0v3yDhA",
        "outputId": "d50f6618-1076-4bf8-941d-97a8f399a3ec"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bagging Regressor Performance (MSE)\n",
            "bootstrap=True, max_samples=1.0 : MSE=0.2559\n",
            "bootstrap=True, max_samples=0.7 : MSE=0.2590\n",
            "bootstrap=False, max_samples=1.0 : MSE=0.4645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VBX67POQyN_v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}